import os
import mlflow
import mlflow.sklearn
import pandas as pd
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel


class IrisFeatures(BaseModel):
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float


app = FastAPI(title="Iris Classifier API")

# Variable globale pour le mod√®le
model = None


def load_model():
    """Charge le dernier mod√®le depuis MLflow"""
    try:
        # Option 1: Si MODEL_DIR est d√©fini (pour production)
        model_dir = os.getenv("MODEL_DIR")
        if model_dir and os.path.exists(model_dir):
            print(f"üì¶ Chargement du mod√®le depuis: {model_dir}")
            return mlflow.sklearn.load_model(model_dir)
        
        # Option 2: Charger depuis MLflow (par d√©faut)
        mlflow.set_tracking_uri("file:///app/mlruns")
        experiment_name = "Pipeline_Iris_Classification"
        
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if experiment is None:
            raise ValueError(f"‚ùå Exp√©rience '{experiment_name}' non trouv√©e dans MLflow")
        
        # R√©cup√©rer la derni√®re run
        runs = mlflow.search_runs(
            experiment_ids=[experiment.experiment_id],
            order_by=["start_time DESC"],
            max_results=1
        )
        
        if runs.empty:
            raise ValueError("‚ùå Aucune run trouv√©e dans l'exp√©rience")
        
        run_id = runs.iloc[0]["run_id"]
        model_uri = f"runs:/{run_id}/model"
        
        print(f"‚úÖ Chargement du mod√®le depuis MLflow: {model_uri}")
        return mlflow.sklearn.load_model(model_uri)
        
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
        raise


@app.on_event("startup")
async def startup_event():
    """Charge le mod√®le au d√©marrage de l'API"""
    global model
    print("üöÄ D√©marrage de l'API Iris Classifier...")
    try:
        model = load_model()
        print("‚úÖ Mod√®le charg√© avec succ√®s !")
    except Exception as e:
        print(f"‚ö†Ô∏è  Impossible de charger le mod√®le: {e}")


@app.get("/")
def root():
    """Page d'accueil"""
    return {
        "message": "Bienvenue sur l'API de classification Iris",
        "description": "Pr√©dit l'esp√®ce d'une fleur Iris",
        "endpoints": {
            "GET /health": "V√©rifier l'√©tat de l'API",
            "POST /predict": "Pr√©dire l'esp√®ce d'une fleur",
            "GET /docs": "Documentation interactive"
        },
        "model_loaded": model is not None
    }


@app.get("/health")
def health():
    """Endpoint de sant√©"""
    return {
        "status": "ok" if model is not None else "model_not_loaded",
        "model_loaded": model is not None
    }


@app.post("/predict")
def predict(features: IrisFeatures):
    """
    Pr√©dit l'esp√®ce d'une fleur Iris
    
    Args:
        features: Caract√©ristiques de la fleur (4 mesures)
    
    Returns:
        Esp√®ce pr√©dite (setosa, versicolor, virginica)
    """
    if model is None:
        raise HTTPException(
            status_code=503,
            detail="Mod√®le non charg√©. V√©rifiez les logs du serveur."
        )
    
    try:
        # Pr√©parer les donn√©es
        data = pd.DataFrame(
            [[features.sepal_length, features.sepal_width, 
              features.petal_length, features.petal_width]],
            columns=["sepal_length", "sepal_width", "petal_length", "petal_width"]
        )
        
        # Pr√©diction
        prediction = model.predict(data)[0]
        
        # Optionnel: obtenir les probabilit√©s
        proba = model.predict_proba(data)[0]
        classes = model.classes_
        
        probabilities = {
            str(cls): float(prob) 
            for cls, prob in zip(classes, proba)
        }
        
        return {
            "species": prediction,
            "probabilities": probabilities
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la pr√©diction: {str(e)}"
        )


@app.post("/batch_predict")
def batch_predict(features_list: list[IrisFeatures]):
    """
    Pr√©dit l'esp√®ce pour plusieurs fleurs √† la fois
    
    Args:
        features_list: Liste de caract√©ristiques de fleurs
    
    Returns:
        Liste de pr√©dictions
    """
    if model is None:
        raise HTTPException(
            status_code=503,
            detail="Mod√®le non charg√©"
        )
    
    try:
        # Pr√©parer les donn√©es
        data = pd.DataFrame([
            [f.sepal_length, f.sepal_width, f.petal_length, f.petal_width]
            for f in features_list
        ], columns=["sepal_length", "sepal_width", "petal_length", "petal_width"])
        
        # Pr√©dictions
        predictions = model.predict(data)
        
        return {
            "count": len(predictions),
            "predictions": [{"species": pred} for pred in predictions]
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erreur: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)